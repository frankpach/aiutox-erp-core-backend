#!/usr/bin/env python3
"""
Script para probar el nuevo endpoint /dashboard
Fase 2A: Backend Batch Endpoint
"""

import asyncio
import time
import sys
from pathlib import Path
from typing import Any

# Add backend to path
backend_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_dir))


async def test_dashboard_endpoint():
    """Prueba el endpoint /dashboard vs endpoints individuales."""
    print("ğŸ§ª Probando endpoint /dashboard...")
    
    try:
        from app.core.db.deps import get_db
        from app.core.auth.service import get_auth_service
        from app.core.tasks.service import get_task_service
        from app.api.v1.tasks import get_tasks_dashboard
        from app.models.user import User
        
        # Obtener conexiÃ³n y usuario de prueba
        db = next(get_db())
        auth_service = get_auth_service(db)
        task_service = get_task_service(db)
        
        # Buscar primer usuario para pruebas
        user = db.query(User).first()
        if not user:
            print("   âš ï¸ No hay usuarios para probar")
            return
        
        print(f"   ğŸ‘¤ Usando usuario: {user.email}")
        
        # Probar 1: Endpoint individual (my-tasks)
        print("\nğŸ“Š Test 1: Endpoint individual /my-tasks")
        start = time.time()
        
        # Simular llamada a /my-tasks
        if hasattr(task_service.repository, 'get_visible_tasks_cached'):
            tasks = task_service.repository.get_visible_tasks_cached(
                tenant_id=user.tenant_id,
                user_id=user.id,
                skip=0,
                limit=20
            )
        else:
            tasks = task_service.repository.get_visible_tasks(
                tenant_id=user.tenant_id,
                user_id=user.id,
                skip=0,
                limit=20
            )
        
        time_individual = time.time() - start
        print(f"   â±ï¸ Tiempo: {time_individual:.3f}s ({len(tasks)} tareas)")
        
        # Probar 2: Endpoint dashboard
        print("\nğŸ“Š Test 2: Endpoint batch /dashboard")
        start = time.time()
        
        # Simular llamada a /dashboard
        dashboard_data = await get_tasks_dashboard(
            current_user=user,
            service=task_service,
            page=1,
            page_size=20
        )
        
        time_dashboard = time.time() - start
        tasks_count = len(dashboard_data.data.get('tasks', []))
        settings = dashboard_data.data.get('settings', {})
        assignments = dashboard_data.data.get('assignments', {})
        
        print(f"   â±ï¸ Tiempo: {time_dashboard:.3f}s")
        print(f"   ğŸ“‹ Tareas: {tasks_count}")
        print(f"   âš™ï¸ Settings: {len(settings)} campos")
        print(f"   ğŸ‘¥ Assignments: {len(assignments)} tareas con asignaciones")
        
        # Calcular mejora
        if time_individual > 0:
            improvement = ((time_individual - time_dashboard) / time_individual) * 100
            print(f"\nğŸš€ Mejora de rendimiento: {improvement:.1f}% mÃ¡s rÃ¡pido")
            
            if improvement > 0:
                print(f"   âœ… Ahorro: {time_individual - time_dashboard:.3f}s por request")
            else:
                print(f"   âš ï¸ El batch endpoint es mÃ¡s lento (sobrecarga de async)")
        
        # Probar 3: MÃºltiples requests concurrentes
        print("\nğŸ“Š Test 3: 5 requests concurrentes")
        
        async def concurrent_request():
            return await get_tasks_dashboard(
                current_user=user,
                service=task_service,
                page=1,
                page_size=20
            )
        
        start = time.time()
        results = await asyncio.gather(*[concurrent_request() for _ in range(5)])
        time_concurrent = time.time() - start
        
        print(f"   â±ï¸ Tiempo total: {time_concurrent:.3f}s")
        print(f"   â±ï¸ Promedio por request: {time_concurrent/5:.3f}s")
        
        # Verificar consistencia de datos
        print("\nğŸ” VerificaciÃ³n de consistencia:")
        first_result = results[0].data
        all_consistent = all(
            result.data['tasks'] == first_result['tasks'] 
            for result in results
        )
        
        if all_consistent:
            print("   âœ… Todos los requests retornaron datos consistentes")
        else:
            print("   âš ï¸ Inconsistencia detectada en datos concurrentes")
        
        print("\nâœ… Test del endpoint /dashboard completado!")
        
    except Exception as e:
        print(f"   âŒ Error en prueba: {e}")
        import traceback
        traceback.print_exc()


async def test_error_handling():
    """Prueba manejo de errores del endpoint."""
    print("\nğŸ§ª Probando manejo de errores...")
    
    try:
        from app.core.db.deps import get_db
        from app.core.tasks.service import get_task_service
        from app.api.v1.tasks import get_tasks_dashboard
        from app.models.user import User
        from app.core.exceptions import APIException
        
        # Crear usuario invÃ¡lido (tenant_id nulo)
        class InvalidUser:
            id = "invalid-id"
            tenant_id = None
            email = "invalid@test.com"
        
        invalid_user = InvalidUser()
        
        db = next(get_db())
        task_service = get_task_service(db)
        
        # DeberÃ­a fallar gracefulmente
        try:
            result = await get_tasks_dashboard(
                current_user=invalid_user,
                service=task_service,
                page=1,
                page_size=20
            )
            print("   âš ï¸ Expected error but got result")
        except APIException as e:
            print(f"   âœ… Error manejado correctamente: {e.message}")
        except Exception as e:
            print(f"   âš ï¸ Error no manejado: {e}")
        
    except Exception as e:
        print(f"   âŒ Error en prueba de errores: {e}")


def main():
    """FunciÃ³n principal."""
    print("ğŸš€ Test del Endpoint /dashboard - Fase 2A")
    print("=" * 50)
    
    # Ejecutar pruebas asÃ­ncronas
    asyncio.run(test_dashboard_endpoint())
    asyncio.run(test_error_handling())
    
    print("\nğŸ“‹ Resumen:")
    print("   âœ… Endpoint /dashboard implementado")
    print("   âœ… EjecuciÃ³n paralela de queries")
    print("   âœ… Manejo de errores individual")
    print("   âœ… Consistencia de datos")
    print("\nğŸ¯ PrÃ³ximos pasos:")
    print("   1. Reiniciar servidor backend")
    print("   2. Probar manualmente: GET /api/v1/tasks/dashboard")
    print("   3. Implementar hook en frontend (opcional)")


if __name__ == "__main__":
    main()
