# Plan Mejorado para Completar y Verificar Todas las Pruebas

**Fecha de Inicio:** [Se completar√° al iniciar]
**√öltima Actualizaci√≥n:** [Se actualizar√° despu√©s de cada test]
**Estado:** üîÑ En Progreso

---

## üìã √çndice

1. [Flujo de Trabajo Completo](#flujo-de-trabajo-completo)
2. [Inicializaci√≥n](#inicializaci√≥n)
3. [Estado Actual](#estado-actual)
4. [Plan de Ejecuci√≥n por M√≥dulo](#plan-de-ejecuci√≥n-por-m√≥dulo)
5. [Seguimiento de Progreso](#seguimiento-de-progreso)
6. [Lista de Errores y Correcciones](#lista-de-errores-y-correcciones)
7. [Manejo de Tests Saltados](#manejo-de-tests-saltados)
8. [Manejo de Warnings](#manejo-de-warnings)
9. [Procedimiento para Retomar](#procedimiento-para-retomar)
10. [Verificaci√≥n Final](#verificaci√≥n-final)
11. [Detecci√≥n de Ciclos Infinitos](#detecci√≥n-de-ciclos-infinitos)
12. [Procedimiento de Actualizaci√≥n del Documento](#procedimiento-de-actualizaci√≥n-del-documento)
13. [Comandos √ötiles](#comandos-√∫tiles)
14. [Archivos Clave](#archivos-clave)
15. [Criterios de √âxito Final](#criterios-de-√©xito-final)
16. [Notas Importantes](#notas-importantes)
17. [Inicio R√°pido](#inicio-r√°pido)

---

## üîÑ Flujo de Trabajo Completo

### Resumen del Procedimiento

1. **Inicializaci√≥n:**
   - Crear archivo `last_test_{datetime}.md`
   - Configurar pytest para mejor retroalimentaci√≥n
   - Ejecutar suite completa para obtener estado inicial

2. **Por Cada M√≥dulo:**
   - Ejecutar tests del m√≥dulo
   - Capturar resultados y errores
   - Actualizar documento de seguimiento
   - Si hay errores: corregirlos inmediatamente
   - Re-ejecutar test para verificar correcci√≥n
   - Detectar ciclos infinitos (si aplica)

3. **Despu√©s de Cada Correcci√≥n:**
   - Actualizar documento marcando error como corregido
   - Documentar soluci√≥n aplicada
   - Verificar que no se crearon nuevos errores

4. **Al Finalizar Todos los M√≥dulos:**
   - Ejecutar suite completa de tests
   - Verificar cobertura
   - Generar reporte final
   - Actualizar documentaci√≥n si es necesario
   - Actualizar reglas si es necesario

### Flujo Visual

```
INICIO
  ‚Üì
Crear last_test_{datetime}.md
  ‚Üì
Ejecutar suite completa (estado inicial)
  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Por cada m√≥dulo en el plan:     ‚îÇ
‚îÇ 1. Ejecutar test del m√≥dulo     ‚îÇ
‚îÇ 2. Actualizar documento          ‚îÇ
‚îÇ 3. ¬øHay errores?                 ‚îÇ
‚îÇ    S√ç ‚Üí Corregir inmediatamente  ‚îÇ
‚îÇ    NO ‚Üí Siguiente m√≥dulo         ‚îÇ
‚îÇ 4. ¬øCiclo detectado?            ‚îÇ
‚îÇ    S√ç ‚Üí Soluci√≥n de fondo       ‚îÇ
‚îÇ    NO ‚Üí Continuar               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚Üì
Ejecutar suite completa (verificaci√≥n final)
  ‚Üì
Generar reporte final
  ‚Üì
FIN
```

---

## üöÄ Inicializaci√≥n

### Paso 1: Crear Archivo de Seguimiento

**Al iniciar la bater√≠a de tests, crear archivo:**
```
backend/tests/analysis/last_test_{datetime}.md
```

**Formato del nombre:** `last_test_YYYYMMDD_HHMMSS.md` (ejemplo: `last_test_20250113_143022.md`)

**Comando para crear archivo:**
```bash
cd backend
uv run python tests/scripts/create_test_tracking.py
```

**O manualmente:**
```bash
cd backend/tests/analysis
# El script crear√° autom√°ticamente el archivo con timestamp
python ../../tests/scripts/create_test_tracking.py
```

**Contenido inicial del archivo:**
- Estado inicial de tests
- Plan completo de ejecuci√≥n
- Lista de m√≥dulos a verificar
- Estructura para seguimiento de errores
- Historial de actualizaciones

### Paso 2: Configuraci√≥n de pytest para Retroalimentaci√≥n

**Mejoras para tests largos sin retroalimentaci√≥n:**

1. **Agregar plugins de pytest para progreso:**
   ```bash
   # Agregar a pyproject.toml en [project.optional-dependencies] dev:
   "pytest-progress>=1.0.0",
   "pytest-timeout>=2.1.0",
   ```

2. **Actualizar configuraci√≥n de pytest en `pyproject.toml`:**
   ```toml
   [tool.pytest.ini_options]
   testpaths = ["tests"]
   python_files = ["test_*.py", "*_test.py"]
   python_classes = ["Test*"]
   python_functions = ["test_*"]
   addopts = "-v --tb=short --durations=10 --timeout=300"
   asyncio_mode = "auto"
   timeout = 300  # 5 minutos por test
   ```

3. **Comando mejorado para ejecuci√≥n con retroalimentaci√≥n:**
   ```bash
   cd backend
   uv run --extra dev pytest -v --tb=short --durations=10 --timeout=300 --progress
   ```

---

## üìä Estado Actual

### Resumen Ejecutivo

- **Tests pasando:** ~688 (89.1%) - [Se actualizar√° despu√©s de cada ejecuci√≥n]
- **Tests fallando:** ~84 (10.9%) - [Se actualizar√° despu√©s de cada ejecuci√≥n]
- **Tests saltados:** ~2 (0.3%) - [Se investigar√° y documentar√° cada uno]
- **Warnings:** ~[N] - [Se capturar√°n y clasificar√°n todos]
  - üî¥ Cr√≠ticas: ~[N]
  - üü° Altas: ~[N]
  - üü¢ Medias: ~[N]
  - ‚ö™ Bajas: ~[N]
- **Errores:** ~1

### Mejoras Ya Implementadas

‚úÖ **Helper de permisos:** `backend/tests/helpers.py` - `create_user_with_permission()`
‚úÖ **Event helpers:** `backend/app/core/pubsub/event_helpers.py` - `safe_publish_event()`
‚úÖ **Correcci√≥n de formato:** `error_code` ‚Üí `code` en 18 archivos
‚úÖ **StandardListResponse:** Corregido en 6 archivos de endpoints
‚úÖ **11 m√≥dulos agregados a MODULE_ROLES**

---

## üì¶ Plan de Ejecuci√≥n por M√≥dulo

### Orden de Ejecuci√≥n (Prioridad)

**Fase 1: M√≥dulos Core/Infraestructura (Objetivo: >90% cobertura)**
1. ‚úÖ **auth** - `test_auth_*.py` (login, me, endpoints, service)
2. ‚ö†Ô∏è **users** - `test_user_management.py`
3. ‚ö†Ô∏è **config** - `test_config.py`
4. ‚ö†Ô∏è **pubsub** - `test_pubsub_*.py` (unit, integration, api)
5. ‚ö†Ô∏è **notifications** - `test_notifications_api.py`
6. ‚ö†Ô∏è **reporting** - `test_reporting_api.py`

**Fase 2: M√≥dulos de Negocio Cr√≠ticos (Objetivo: >80% cobertura)**
7. ‚ö†Ô∏è **products** - `test_products.py`, `test_products_events.py`
8. ‚úÖ **tags** - `test_tags_api.py` (8 tests)
9. ‚úÖ **tasks** - `test_tasks_api.py` (7 tests)
10. ‚úÖ **files** - `test_files_api.py` (6 tests)
11. ‚úÖ **activities** - `test_activities_api.py` (6 tests)
12. ‚úÖ **workflows** - `test_workflows_api.py` (7 tests)
13. ‚úÖ **integrations** - `test_integrations_api.py` (11 tests)
14. ‚úÖ **preferences** - `test_preferences_api.py` (7 tests)

**Fase 3: M√≥dulos de Negocio Secundarios (Objetivo: >80% cobertura)**
15. ‚ö†Ô∏è **calendar** - `test_calendar_api.py`, `test_calendar_integration.py`
16. ‚ö†Ô∏è **comments** - `test_comments_api.py`, `test_comments_integration.py`
17. ‚ö†Ô∏è **approvals** - `test_approvals_api.py`, `test_approvals_integration.py`
18. ‚ö†Ô∏è **templates** - `test_templates_api.py`, `test_templates_integration.py`
19. ‚ö†Ô∏è **import_export** - `test_import_export_api.py`, `test_import_export_integration.py`
20. ‚ö†Ô∏è **views** - `test_views_api.py`, `test_views_integration.py`
21. ‚ö†Ô∏è **automation** - `test_automation_api.py`, `test_automation_engine.py`
22. ‚ö†Ô∏è **search** - `test_search_api.py`

**Fase 4: Tests de Infraestructura y Seguridad**
23. ‚ö†Ô∏è **rbac** - `test_rbac.py`
24. ‚ö†Ô∏è **security** - `test_security_multi_tenant.py`
25. ‚ö†Ô∏è **audit** - `test_audit_logs.py`
26. ‚ö†Ô∏è **error_handling** - `test_error_handling.py`
27. ‚ö†Ô∏è **standard_responses** - `test_standard_responses.py`

**Fase 5: Tests Unitarios**
28. ‚ö†Ô∏è **unit/** - Todos los tests unitarios
29. ‚ö†Ô∏è **cli/** - Tests del CLI

---

## üìà Seguimiento de Progreso

### Estructura de Seguimiento por M√≥dulo

Para cada m√≥dulo, registrar:

```markdown
### M√≥dulo: [nombre]

**Archivo de test:** `tests/integration/test_[nombre]_api.py`
**Estado:** ‚è≥ Pendiente | üîÑ En Progreso | ‚úÖ Completado | ‚ùå Error
**√öltima ejecuci√≥n:** [timestamp]
**Resultado:**
- Tests totales: [N]
- Tests pasando: [N]
- Tests fallando: [N]
- Tests saltados: [N]
- **Warnings:** [N] ‚ö†Ô∏è
  - üî¥ Cr√≠ticas: [N]
  - üü° Altas: [N]
  - üü¢ Medias: [N]
  - ‚ö™ Bajas: [N]
- Tiempo de ejecuci√≥n: [X]s

**Errores encontrados:**
1. [Descripci√≥n del error] - Estado: ‚è≥ Pendiente | ‚úÖ Corregido
2. [Descripci√≥n del error] - Estado: ‚è≥ Pendiente | ‚úÖ Corregido

**Tests saltados:**
1. `test_nombre` - Raz√≥n: [Raz√≥n] - Tipo: ‚úÖ Intencional | ‚ùå Problema - Acci√≥n: [Mantener | Corregir]

**Warnings encontrados:**
1. [Warning cr√≠tico] - Severidad: üî¥ - Estado: ‚è≥ Pendiente | ‚úÖ Corregido | üìù Aceptado (raz√≥n: [raz√≥n])
2. [Warning alta] - Severidad: üü° - Estado: ‚è≥ Pendiente | ‚úÖ Corregido | üìù Aceptado (raz√≥n: [raz√≥n])

**Acciones realizadas:**
- [Timestamp] - [Acci√≥n realizada]
- [Timestamp] - [Acci√≥n realizada]
- [Timestamp] - Investigado test saltado: [nombre]
- [Timestamp] - Clasificado warning: [descripci√≥n]

**Pr√≥ximas acciones:**
- [ ] [Acci√≥n pendiente]
```

---

## üêõ Lista de Errores y Correcciones

### Categor√≠as de Errores

#### 1. Errores de Permisos (403 Forbidden)
**Patr√≥n:** Tests que fallan con `assert 403 == 201` o `assert 403 == 200`

**Soluci√≥n est√°ndar:**
```python
# ANTES
def test_example(client, test_user, auth_headers, db_session):
    response = client.post("/api/v1/endpoint", json=data, headers=auth_headers)

# DESPU√âS
def test_example(client, test_user, db_session):
    headers = create_user_with_permission(db_session, test_user, "module_name", "manager")
    response = client.post("/api/v1/endpoint", json=data, headers=headers)
```

**Lista de errores:**
- [ ] `test_create_tag` - 403 Forbidden - ‚è≥ Pendiente
- [ ] `test_create_task` - 403 Forbidden - ‚è≥ Pendiente
- [ ] `test_upload_file` - 403 Forbidden - ‚è≥ Pendiente
- [ ] `test_get_report` - 403 Forbidden - ‚è≥ Pendiente
- [ ] `test_create_report` - 403 Forbidden - ‚è≥ Pendiente
- [ ] `test_save_view` - 403 Forbidden - ‚è≥ Pendiente
- [ ] `test_create_dashboard` - 403 Forbidden - ‚è≥ Pendiente
- [ ] `test_index_entity` - 403 Forbidden - ‚è≥ Pendiente
- [ ] `test_get_suggestions` - 403 Forbidden - ‚è≥ Pendiente

#### 2. Errores de Formato de Respuesta
**Patr√≥n:** `AssertionError` relacionado con estructura de respuesta

**Soluci√≥n est√°ndar:**
- Verificar que endpoints usen `StandardResponse` o `StandardListResponse`
- Eliminar `success=True` de respuestas
- Verificar que errores usen `code` en lugar de `error_code`

**Lista de errores:**
- [ ] `test_login_success` - Formato de respuesta - ‚è≥ Pendiente
- [ ] `test_list_roles_returns_standard_list_response` - Formato - ‚è≥ Pendiente

#### 3. Errores de Event Loop PubSub
**Patr√≥n:** "There is no current event loop" o "Failed to publish [event].created event"

**Soluci√≥n est√°ndar:**
```python
# Usar safe_publish_event en lugar de publish_event directamente
from app.core.pubsub.event_helpers import safe_publish_event

safe_publish_event("module.entity.created", {"entity_id": entity.id})
```

**Lista de errores:**
- [ ] Activities - Event loop - ‚è≥ Pendiente
- [ ] Tasks - Event loop - ‚è≥ Pendiente
- [ ] Calendar - Event loop - ‚è≥ Pendiente
- [ ] Comments - Event loop - ‚è≥ Pendiente
- [ ] Templates - Event loop - ‚è≥ Pendiente
- [ ] Import/Export - Event loop - ‚è≥ Pendiente
- [ ] Products - Event loop - ‚è≥ Pendiente

#### 4. Errores de Base de Datos
**Patr√≥n:** `sqlalchemy.exc.ProgrammingError`, `sqlalchemy.exc.InternalError`, `psycopg2.errors.InFailedSqlTransaction`

**Soluci√≥n est√°ndar:**
- Verificar cleanup de transacciones
- Asegurar que `db_session.refresh()` se llame despu√©s de commits
- Verificar que no haya transacciones abiertas

**Lista de errores:**
- [ ] Tags - DB transaction - ‚è≥ Pendiente
- [ ] Tasks - DB transaction - ‚è≥ Pendiente
- [ ] Workflows - DB transaction - ‚è≥ Pendiente
- [ ] Files - DB transaction - ‚è≥ Pendiente
- [ ] Integrations - DB transaction - ‚è≥ Pendiente

#### 5. Errores de Validaci√≥n/Esquemas
**Patr√≥n:** `AttributeError`, `TypeError: 'NoneType' object`, validaci√≥n de schemas fallida

**Soluci√≥n est√°ndar:**
- Verificar que servicios tengan los m√©todos necesarios
- Verificar que objetos no sean None antes de usar
- Revisar validaciones de schemas

**Lista de errores:**
- [ ] Tags - Validaci√≥n - ‚è≥ Pendiente
- [ ] Tasks - Validaci√≥n - ‚è≥ Pendiente
- [ ] Notifications - Validaci√≥n - ‚è≥ Pendiente
- [ ] Templates - Validaci√≥n - ‚è≥ Pendiente

---

## ‚è≠Ô∏è Manejo de Tests Saltados

### Procedimiento Obligatorio para Tests Saltados

**IMPORTANTE:** Todos los tests saltados deben ser investigados y documentados. No se puede dejar ning√∫n test saltado sin justificaci√≥n expl√≠cita.

### Paso 1: Identificar Tests Saltados

Despu√©s de cada ejecuci√≥n de tests, identificar todos los tests marcados como `SKIPPED`:

```bash
# Capturar tests saltados
uv run --extra dev pytest -v --tb=no | grep -i "skipped"
```

### Paso 2: Investigar la Raz√≥n del Skip

Para cada test saltado, determinar la raz√≥n:

1. **Revisar el c√≥digo del test:**
   ```python
   @pytest.mark.skip(reason="...")  # Raz√≥n expl√≠cita
   @pytest.mark.skipif(condition, reason="...")  # Condici√≥n
   ```

2. **Verificar si es intencional:**
   - ¬øEl test est√° marcado con `@pytest.mark.skip` con una raz√≥n clara?
   - ¬øEs un test que requiere condiciones espec√≠ficas (ej: Redis, servicios externos)?
   - ¬øEs un test temporalmente deshabilitado?

3. **Verificar si es un problema:**
   - ¬øEl test falla y fue saltado para ocultar el error?
   - ¬øFalta alguna dependencia o configuraci√≥n?
   - ¬øHay un problema de infraestructura?

### Paso 3: Documentar en `last_test_{datetime}.md`

**Para cada test saltado, agregar entrada en el archivo de seguimiento:**

```markdown
### Tests Saltados - M√≥dulo: [nombre]

#### Test: `test_nombre_del_test`
- **Archivo:** `tests/integration/test_[module]_api.py::test_nombre_del_test`
- **Raz√≥n del skip:** [Raz√≥n encontrada]
- **Tipo:**
  - ‚úÖ Intencional (requiere condici√≥n espec√≠fica)
  - ‚ùå Problema (debe corregirse)
- **Acci√≥n requerida:**
  - [ ] Mantener saltado (si es intencional)
  - [ ] Corregir y habilitar (si es problema)
- **Justificaci√≥n:** [Explicaci√≥n detallada]
- **Fecha de revisi√≥n:** [YYYY-MM-DD HH:MM:SS]
```

### Paso 4: Decidir Acci√≥n

**Si es INTENCIONAL:**
- ‚úÖ Documentar raz√≥n clara en el c√≥digo del test
- ‚úÖ Asegurar que el `reason` del `@pytest.mark.skip` sea descriptivo
- ‚úÖ Verificar que la condici√≥n sea v√°lida (ej: `@pytest.mark.skipif(not redis_available, reason="Requires Redis")`)
- ‚úÖ Mantener el test saltado
- ‚úÖ Documentar en `last_test_{datetime}.md` como intencional

**Si es un PROBLEMA:**
- ‚ùå NO dejar el test saltado sin corregir
- ‚ùå Investigar y corregir la causa ra√≠z
- ‚ùå Habilitar el test despu√©s de la correcci√≥n
- ‚ùå Verificar que el test pase
- ‚ùå Documentar la correcci√≥n en `last_test_{datetime}.md`

### Paso 5: Actualizar C√≥digo del Test

**Para tests intencionales, asegurar que tengan raz√≥n clara:**

```python
# ‚úÖ CORRECTO - Raz√≥n clara
@pytest.mark.skipif(
    not redis_available,
    reason="Test requires Redis connection. Run with Redis available."
)

# ‚úÖ CORRECTO - Test temporalmente deshabilitado con raz√≥n
@pytest.mark.skip(
    reason="Temporarily disabled due to external API changes. TODO: Update test after API migration."
)

# ‚ùå INCORRECTO - Sin raz√≥n
@pytest.mark.skip()

# ‚ùå INCORRECTO - Raz√≥n vaga
@pytest.mark.skip(reason="Doesn't work")
```

### Criterios de √âxito para Tests Saltados

- ‚úÖ Todos los tests saltados tienen raz√≥n documentada
- ‚úÖ Todos los tests saltados est√°n clasificados (intencional vs problema)
- ‚úÖ Todos los tests saltados est√°n documentados en `last_test_{datetime}.md`
- ‚úÖ Tests saltados por problemas han sido corregidos o est√°n en proceso
- ‚úÖ El informe final incluye secci√≥n expl√≠cita sobre tests saltados

### Ejemplo de Documentaci√≥n en `last_test_{datetime}.md`

```markdown
## üìä Resumen de Tests Saltados

**Total de tests saltados:** [N]
**Tests intencionales:** [N]
**Tests con problemas:** [N]

### Detalle por M√≥dulo

#### M√≥dulo: auth
- `test_redis_rate_limiting` - ‚úÖ Intencional - Requiere Redis
- `test_external_api_integration` - ‚ùå Problema - API externa no disponible

#### M√≥dulo: products
- `test_import_large_file` - ‚úÖ Intencional - Requiere archivo de prueba grande

### Acciones Pendientes
- [ ] Corregir `test_external_api_integration` en m√≥dulo auth
- [ ] Verificar que todos los tests intencionales tienen raz√≥n clara
```

---

## ‚ö†Ô∏è Manejo de Warnings

### Procedimiento Obligatorio para Warnings

**IMPORTANTE:** Todos los warnings deben ser capturados, clasificados y documentados. Warnings de alta severidad requieren acci√≥n inmediata.

### Paso 1: Capturar Warnings

**Configurar pytest para capturar warnings:**

```bash
# Ejecutar tests con captura de warnings
uv run --extra dev pytest -v --tb=short -W default::Warning
```

**O con configuraci√≥n en `pyproject.toml`:**
```toml
[tool.pytest.ini_options]
filterwarnings = [
    "error",  # Convertir warnings cr√≠ticos en errores
    "ignore::DeprecationWarning:app",  # Ignorar warnings espec√≠ficos solo si es necesario
    "default",  # Mostrar todos los dem√°s warnings
]
```

### Paso 2: Clasificar Warnings por Severidad

**Categor√≠as de severidad:**

#### üî¥ **CR√çTICA** (Requiere acci√≥n inmediata)
- Warnings de seguridad (ej: uso de funciones inseguras)
- Warnings de deprecaci√≥n en c√≥digo cr√≠tico
- Warnings de configuraci√≥n incorrecta
- Warnings que pueden causar errores en producci√≥n

#### üü° **ALTA** (Requiere acci√≥n en corto plazo)
- Warnings de deprecaci√≥n en c√≥digo activo
- Warnings de rendimiento
- Warnings de compatibilidad futura
- Warnings de buenas pr√°cticas

#### üü¢ **MEDIA** (Recomendado corregir)
- Warnings de estilo de c√≥digo
- Warnings informativos
- Warnings de optimizaci√≥n menor

#### ‚ö™ **BAJA** (Opcional)
- Warnings cosm√©ticos
- Warnings de librer√≠as externas (no controlables)
- Warnings informativos sin impacto

### Paso 3: Registrar en `last_test_{datetime}.md`

**Para cada warning, agregar entrada:**

```markdown
### Warnings - M√≥dulo: [nombre]

#### Warning: [Descripci√≥n]
- **Tipo:** [DeprecationWarning, PendingDeprecationWarning, UserWarning, etc.]
- **Severidad:** üî¥ Cr√≠tica | üü° Alta | üü¢ Media | ‚ö™ Baja
- **Ubicaci√≥n:** `app/module/file.py:line`
- **Mensaje completo:** [Mensaje del warning]
- **Acci√≥n requerida:**
  - [ ] Corregir inmediatamente (si es cr√≠tica)
  - [ ] Planificar correcci√≥n (si es alta/media)
  - [ ] Documentar como aceptable (si es baja)
- **Fecha de detecci√≥n:** [YYYY-MM-DD HH:MM:SS]
- **Estado:** ‚è≥ Pendiente | üîÑ En Progreso | ‚úÖ Corregido | üìù Aceptado
```

### Paso 4: Definir Cu√°ndo un Warning Requiere Acci√≥n

**Warnings que SIEMPRE requieren acci√≥n:**

1. **Warnings de seguridad:**
   - Uso de funciones inseguras
   - Configuraciones de seguridad incorrectas
   - Exposici√≥n de informaci√≥n sensible

2. **Warnings de deprecaci√≥n en c√≥digo cr√≠tico:**
   - Funciones que se eliminar√°n en pr√≥xima versi√≥n
   - APIs que cambiar√°n y afectan funcionalidad core

3. **Warnings de configuraci√≥n:**
   - Variables de entorno faltantes
   - Configuraciones incorrectas que pueden causar errores

**Warnings que pueden documentarse como aceptables:**

1. **Warnings de librer√≠as externas:**
   - Warnings de dependencias de terceros que no controlamos
   - Warnings conocidos sin soluci√≥n disponible

2. **Warnings informativos:**
   - Warnings que no afectan funcionalidad
   - Warnings cosm√©ticos

### Paso 5: Incluir en Seguimiento de Progreso

**Actualizar secci√≥n de seguimiento por m√≥dulo:**

```markdown
### M√≥dulo: [nombre]

**Resultado:**
- Tests totales: [N]
- Tests pasando: [N] ‚úÖ
- Tests fallando: [N] ‚ùå
- Tests saltados: [N] ‚è≠Ô∏è
- **Warnings:** [N] ‚ö†Ô∏è
  - üî¥ Cr√≠ticas: [N]
  - üü° Altas: [N]
  - üü¢ Medias: [N]
  - ‚ö™ Bajas: [N]
- Tiempo de ejecuci√≥n: [X]s

**Warnings encontrados:**
1. [Warning cr√≠tico] - Estado: ‚è≥ Pendiente | ‚úÖ Corregido
2. [Warning alta] - Estado: ‚è≥ Pendiente | ‚úÖ Corregido
```

### Criterios de √âxito para Warnings

- ‚úÖ Todos los warnings est√°n capturados y registrados
- ‚úÖ Todos los warnings est√°n clasificados por severidad
- ‚úÖ Warnings cr√≠ticas han sido corregidas o est√°n en proceso
- ‚úÖ Warnings altas tienen plan de correcci√≥n documentado
- ‚úÖ El informe final incluye secci√≥n expl√≠cita sobre warnings
- ‚úÖ Si no se hizo nada con un warning, la raz√≥n est√° expl√≠citamente documentada

### Ejemplo de Documentaci√≥n en `last_test_{datetime}.md`

```markdown
## ‚ö†Ô∏è Resumen de Warnings

**Total de warnings:** [N]
**Warnings cr√≠ticas:** [N] üî¥
**Warnings altas:** [N] üü°
**Warnings medias:** [N] üü¢
**Warnings bajas:** [N] ‚ö™

### Warnings Cr√≠ticas (Acci√≥n Requerida)

1. **DeprecationWarning en `app/core/auth/jwt.py:45`**
   - **Mensaje:** `jwt.encode()` is deprecated, use `jwt.encode_unsafe()` instead
   - **Acci√≥n:** Actualizar a nueva API de JWT
   - **Estado:** üîÑ En Progreso
   - **Fecha l√≠mite:** [YYYY-MM-DD]

### Warnings Aceptadas (Con Justificaci√≥n)

1. **UserWarning en `app/core/integrations/external_api.py:120`**
   - **Mensaje:** External API library shows warning about rate limits
   - **Raz√≥n de aceptaci√≥n:** Warning de librer√≠a externa, no controlable. Ya manejamos rate limiting en nuestro c√≥digo.
   - **Estado:** üìù Aceptado
   - **Fecha de revisi√≥n:** [YYYY-MM-DD]
```

---

## üîÑ Procedimiento para Retomar

### Si el Proceso se Interrumpe

1. **Leer el archivo `last_test_{datetime}.md` m√°s reciente:**
   ```bash
   ls -lt backend/tests/analysis/last_test_*.md | head -1
   ```

2. **Identificar el √∫ltimo m√≥dulo procesado:**
   - Buscar en el documento la secci√≥n "Seguimiento de Progreso"
   - Encontrar el √∫ltimo m√≥dulo con estado "‚úÖ Completado" o "üîÑ En Progreso"

3. **Continuar desde el siguiente m√≥dulo:**
   - Ejecutar el m√≥dulo siguiente en el orden establecido
   - Actualizar el documento con los resultados

4. **Verificar errores pendientes:**
   - Revisar la secci√≥n "Lista de Errores y Correcciones"
   - Continuar corrigiendo errores pendientes

### Comando para Retomar

```bash
# 1. Leer el √∫ltimo archivo de seguimiento
cat backend/tests/analysis/last_test_*.md | head -50

# 2. Continuar desde el m√≥dulo siguiente
# [Ejecutar el m√≥dulo siguiente seg√∫n el plan]
```

---

## ‚úÖ Verificaci√≥n Final

### Antes de Dar por Terminado

**Paso 1: Ejecutar Suite Completa de Tests**
```bash
cd backend
uv run --extra dev pytest -v --tb=short --durations=10 --timeout=300 -W default::Warning
```

**Paso 2: Verificar Cobertura**
```bash
uv run --extra dev pytest --cov=app --cov-report=html --cov-report=term-missing
```

**Paso 3: Verificar Criterios de √âxito**
- [ ] Todos los tests pasan (0 fallos)
- [ ] Cobertura >90% para m√≥dulos core (auth, permissions, multi-tenancy)
- [ ] Cobertura >80% para m√≥dulos de negocio
- [ ] Todos los endpoints API tienen tests de integraci√≥n
- [ ] Todos los servicios cr√≠ticos tienen tests unitarios
- [ ] Tests validan formato de respuestas seg√∫n API contract
- [ ] Tests incluyen casos edge y validaciones de seguridad
- [ ] **Todos los tests saltados est√°n documentados y justificados**
- [ ] **Tests saltados por problemas han sido corregidos**
- [ ] **Todos los warnings est√°n capturados y clasificados**
- [ ] **Warnings cr√≠ticas han sido corregidas o tienen plan documentado**
- [ ] **Si no se hizo nada con un warning/test saltado, la raz√≥n est√° expl√≠citamente documentada**

**Paso 4: Generar Reporte Final**
```bash
# Crear reporte final
cat > backend/tests/analysis/test_verification_report.md << EOF
# Reporte Final de Verificaci√≥n de Tests

**Fecha:** $(date +%Y-%m-%d\ %H:%M:%S)
**Estado:** ‚úÖ Completado

## Resumen
- Tests totales: [N]
- Tests pasando: [N]
- Tests fallando: [N]
- **Tests saltados: [N]**
  - Tests intencionales: [N]
  - Tests con problemas: [N]
- **Warnings: [N]**
  - üî¥ Cr√≠ticas: [N]
  - üü° Altas: [N]
  - üü¢ Medias: [N]
  - ‚ö™ Bajas: [N]
- Cobertura general: [X]%
- Cobertura core: [X]%
- Cobertura negocio: [X]%

## M√≥dulos Verificados
[Lista de m√≥dulos con estado]

## Tests Saltados

### Resumen
- **Total:** [N]
- **Intencionales:** [N] (documentados y justificados)
- **Con problemas:** [N] (corregidos o en proceso)

### Detalle
[Lista detallada de cada test saltado con raz√≥n y acci√≥n tomada]

### Acciones Realizadas
- [ ] Todos los tests saltados investigados
- [ ] Todos los tests saltados documentados en `last_test_{datetime}.md`
- [ ] Tests saltados por problemas corregidos o tienen plan de correcci√≥n
- [ ] Tests intencionales tienen raz√≥n clara en c√≥digo

### Justificaci√≥n de Tests Saltados Sin Acci√≥n
**Si alg√∫n test saltado no fue corregido, explicar expl√≠citamente la raz√≥n:**
- Test: `test_nombre`
- Raz√≥n del skip: [Raz√≥n]
- Por qu√© no se corrigi√≥: [Explicaci√≥n detallada]
- Plan futuro: [Si aplica]

## Warnings

### Resumen
- **Total:** [N]
- **Cr√≠ticas:** [N] (corregidas: [N], pendientes: [N])
- **Altas:** [N] (corregidas: [N], pendientes: [N])
- **Medias:** [N] (corregidas: [N], aceptadas: [N])
- **Bajas:** [N] (aceptadas: [N])

### Detalle
[Lista detallada de warnings con severidad y acci√≥n tomada]

### Acciones Realizadas
- [ ] Todos los warnings capturados y registrados
- [ ] Todos los warnings clasificados por severidad
- [ ] Warnings cr√≠ticas corregidas o tienen plan documentado
- [ ] Warnings documentadas en `last_test_{datetime}.md`

### Justificaci√≥n de Warnings Sin Acci√≥n
**Si alg√∫n warning no fue corregido, explicar expl√≠citamente la raz√≥n:**
- Warning: [Descripci√≥n]
- Severidad: [Cr√≠tica/Alta/Media/Baja]
- Por qu√© no se corrigi√≥: [Explicaci√≥n detallada]
- Impacto de no corregir: [An√°lisis de impacto]
- Plan futuro: [Si aplica]

## Recomendaciones
[Recomendaciones finales]
EOF
```

---

## üîÅ Detecci√≥n de Ciclos Infinitos

### Procedimiento para Detectar y Resolver Ciclos

**Definici√≥n de Ciclo Infinito:**
- Mismo error aparece 3+ veces despu√©s de intentos de correcci√≥n
- Correcci√≥n aplicada pero error persiste o cambia a otro error relacionado
- M√∫ltiples correcciones en el mismo archivo sin resolver el problema
- Mismo patr√≥n de error-cambio-error se repite

**Procedimiento de Detecci√≥n:**

1. **Registrar intentos de correcci√≥n en el documento:**
   ```markdown
   ### Error: [Descripci√≥n]
   - Intento 1: [Timestamp] - [Acci√≥n] - ‚ùå Fall√≥
   - Intento 2: [Timestamp] - [Acci√≥n] - ‚ùå Fall√≥
   - Intento 3: [Timestamp] - [Acci√≥n] - ‚ùå Fall√≥
   - **DECISI√ìN:** üî¥ Ciclo detectado - Pasar a soluci√≥n de fondo
   ```

2. **Cuando se detecta un ciclo (despu√©s de 3 intentos):**
   - **DETENER** correcciones iterativas inmediatamente
   - **MARCAR** error como üî¥ Ciclo detectado en el documento
   - **ANALIZAR** la causa ra√≠z del problema (no solo s√≠ntomas)
   - **DISE√ëAR** soluci√≥n de fondo (no parches)
   - **DOCUMENTAR** an√°lisis y soluci√≥n de fondo en el archivo de seguimiento
   - **IMPLEMENTAR** soluci√≥n de fondo
   - **VERIFICAR** que la soluci√≥n resuelve el problema completamente
   - **ACTUALIZAR** documento marcando ciclo como resuelto

**Indicadores de Ciclo:**
- ‚úÖ Error aparece 3+ veces con misma descripci√≥n
- ‚úÖ M√∫ltiples archivos modificados para "corregir" el mismo error
- ‚úÖ Error cambia de forma pero persiste (ej: 403 ‚Üí 500 ‚Üí 403)
- ‚úÖ Correcciones aplicadas pero tests siguen fallando
- ‚úÖ Mismo patr√≥n en m√∫ltiples m√≥dulos

**Ejemplo de Soluci√≥n de Fondo:**

```markdown
### üî¥ Ciclo Detectado: Error de Permisos en M√∫ltiples Tests

**Problema:** M√∫ltiples tests fallan con 403 despu√©s de aplicar create_user_with_permission

**Historial de Intentos:**
- Intento 1: [2025-01-13 10:00:00] - Agregar ModuleRole manualmente - ‚ùå Fall√≥
- Intento 2: [2025-01-13 10:15:00] - Usar create_user_with_permission - ‚ùå Fall√≥
- Intento 3: [2025-01-13 10:30:00] - Refrescar usuario despu√©s de commit - ‚ùå Fall√≥
- **DECISI√ìN:** üî¥ Ciclo detectado - Pasar a soluci√≥n de fondo

**An√°lisis de Causa Ra√≠z:**
1. El helper create_user_with_permission no est√° refrescando correctamente los permisos
2. El token generado no incluye los nuevos permisos porque el usuario no se refresca
3. La cach√© de permisos en el servicio de auth no se est√° limpiando
4. El token JWT se genera antes de que los permisos est√©n disponibles

**Soluci√≥n de Fondo:**
1. Modificar `create_user_with_permission` en `backend/tests/helpers.py`:
   - Forzar refresh completo del usuario desde DB
   - Limpiar cach√© de permisos antes de generar token
   - Verificar que permisos est√©n en el usuario antes de crear token

2. Modificar `AuthService.create_access_token_for_user`:
   - Asegurar que siempre lea permisos frescos de DB
   - No usar cach√© de permisos para tokens de test

3. Agregar fixture para limpiar cach√© de permisos antes de cada test

**Implementaci√≥n:**
[Detalles espec√≠ficos de c√≥digo modificado]

**Archivos Modificados:**
- `backend/tests/helpers.py` - L√≠nea X: [Cambio]
- `backend/app/services/auth_service.py` - L√≠nea Y: [Cambio]
- `backend/tests/conftest.py` - L√≠nea Z: [Cambio]

**Verificaci√≥n:**
- [x] Tests pasan despu√©s de la soluci√≥n
- [x] No se detectan m√°s ciclos relacionados
- [x] Soluci√≥n aplicada a todos los m√≥dulos afectados
```

**Regla de Oro:**
> Si despu√©s de 3 intentos el error persiste, **DETENER** y pasar a soluci√≥n de fondo.
> No continuar con correcciones iterativas que no resuelven el problema ra√≠z.

---

## üìù Procedimiento de Actualizaci√≥n del Documento

### Despu√©s de Cada Test de M√≥dulo

**Paso 1: Ejecutar Test del M√≥dulo**
```bash
cd backend
uv run --extra dev pytest tests/integration/test_[module]_api.py -v --tb=short --durations=10 --timeout=300 -W default::Warning
```

**Paso 2: Capturar Resultados**
- Copiar salida completa del comando
- Extraer estad√≠sticas (passed, failed, skipped)
- Capturar warnings (usar `-W default::Warning`)
- Identificar errores espec√≠ficos
- Identificar tests saltados
- Clasificar warnings por severidad

**Paso 3: Actualizar Archivo de Seguimiento**

**Ubicaci√≥n:** `backend/tests/analysis/last_test_{datetime}.md`

**Opci√≥n A: Usar Script Autom√°tico (Recomendado)**
```bash
cd backend
# Ejecutar test y capturar salida (incluyendo warnings)
uv run --extra dev pytest tests/integration/test_[module]_api.py -v --tb=short -W default::Warning > test_output.txt 2>&1

# Actualizar archivo de seguimiento
uv run python tests/scripts/update_test_tracking.py \
  --module "[module_name]" \
  --test-file "tests/integration/test_[module]_api.py" \
  --output "$(cat test_output.txt)" \
  --errors "Error 1" "Error 2" \
  --actions "Ejecutado test" "Aplicada correcci√≥n X"
```

**Opci√≥n B: Actualizaci√≥n Manual**

**Actualizar secciones:**

1. **Actualizar "Seguimiento de Progreso por M√≥dulo":**
   ```markdown
   ### M√≥dulo: [nombre]

   **Archivo de test:** `tests/integration/test_[nombre]_api.py`
   **Estado:** ‚úÖ Completado
   **√öltima ejecuci√≥n:** [YYYY-MM-DD HH:MM:SS]
   **Resultado:**
   - Tests totales: [N]
   - Tests pasando: [N] ‚úÖ
   - Tests fallando: [N] ‚ùå
   - Tests saltados: [N] ‚è≠Ô∏è
   - **Warnings:** [N] ‚ö†Ô∏è
     - üî¥ Cr√≠ticas: [N]
     - üü° Altas: [N]
     - üü¢ Medias: [N]
     - ‚ö™ Bajas: [N]
   - Tiempo de ejecuci√≥n: [X]s

   **Errores encontrados:**
   1. [Descripci√≥n del error] - Estado: ‚è≥ Pendiente
   2. [Descripci√≥n del error] - Estado: ‚è≥ Pendiente

   **Tests saltados:**
   1. `test_nombre` - Raz√≥n: [Raz√≥n] - Tipo: ‚úÖ Intencional | ‚ùå Problema
   - Acci√≥n: [Mantener | Corregir]

   **Warnings encontrados:**
   1. [Warning cr√≠tico] - Severidad: üî¥ - Estado: ‚è≥ Pendiente | ‚úÖ Corregido
   2. [Warning alta] - Severidad: üü° - Estado: ‚è≥ Pendiente | ‚úÖ Corregido

   **Acciones realizadas:**
   - [Timestamp] - Ejecutado test del m√≥dulo
   - [Timestamp] - [Acci√≥n de correcci√≥n si aplica]
   - [Timestamp] - Investigado test saltado: [nombre]
   - [Timestamp] - Clasificado warning: [descripci√≥n]

   **Pr√≥ximas acciones:**
   - [ ] [Acci√≥n pendiente]
   ```

2. **Actualizar "Lista de Errores y Correcciones":**
   - Agregar nuevos errores encontrados
   - Actualizar estado de errores corregidos (‚è≥ Pendiente ‚Üí ‚úÖ Corregido)

3. **Actualizar "Historial de Actualizaciones":**
   ```markdown
   ### [YYYY-MM-DD HH:MM:SS] - M√≥dulo: [nombre]
   - Ejecutado test del m√≥dulo [nombre]
   - Resultado: [N] pasando, [N] fallando
   - Errores encontrados: [Lista]
   - Acciones: [Acciones realizadas]
   ```

**Paso 4: Si Hay Errores, Corregirlos Inmediatamente**

1. **Analizar error:**
   - Identificar tipo de error (permisos, formato, DB, eventos, validaci√≥n)
   - Buscar patr√≥n similar en otros m√≥dulos
   - Verificar si ya existe soluci√≥n conocida

2. **Aplicar correcci√≥n:**
   - Implementar soluci√≥n seg√∫n patr√≥n est√°ndar
   - Verificar que la correcci√≥n no rompa otros tests

3. **Re-ejecutar test:**
   ```bash
   uv run --extra dev pytest tests/integration/test_[module]_api.py -v
   ```

4. **Actualizar documento:**
   - Marcar error como ‚úÖ Corregido
   - Documentar la soluci√≥n aplicada
   - Actualizar estad√≠sticas

**Paso 5: Detectar Ciclos Infinitos**

Si el mismo error persiste despu√©s de 3 intentos de correcci√≥n:
- **DETENER** correcciones iterativas inmediatamente
- Marcar como üî¥ Ciclo detectado en el documento
- Pasar a soluci√≥n de fondo
- Documentar an√°lisis de causa ra√≠z
- Implementar soluci√≥n de fondo
- Verificar que resuelve el problema
- Actualizar documento con soluci√≥n de fondo

**Importante:** No continuar con m√°s de 3 intentos de correcci√≥n iterativa.
Si despu√©s de 3 intentos el error persiste, es necesario analizar la causa ra√≠z y dise√±ar una soluci√≥n de fondo.

### Plantilla de Actualizaci√≥n

```markdown
## Actualizaci√≥n: [YYYY-MM-DD HH:MM:SS]

### M√≥dulo: [nombre] - [Estado]

**Resultado de ejecuci√≥n:**
```
[Salida completa del comando pytest]
```

**Resumen:**
- Tests totales: [N]
- Tests pasando: [N] ‚úÖ
- Tests fallando: [N] ‚ùå
- Tests saltados: [N] ‚è≠Ô∏è
- **Warnings:** [N] ‚ö†Ô∏è (üî¥ [N] | üü° [N] | üü¢ [N] | ‚ö™ [N])
- Tiempo: [X]s

**Errores encontrados:**
1. [Error 1] - Estado: ‚è≥ Pendiente | ‚úÖ Corregido | üî¥ Ciclo detectado
2. [Error 2] - Estado: ‚è≥ Pendiente | ‚úÖ Corregido | üî¥ Ciclo detectado

**Tests saltados:**
1. `test_nombre` - Raz√≥n: [Raz√≥n] - Tipo: ‚úÖ Intencional | ‚ùå Problema - Acci√≥n: [Mantener | Corregir]

**Warnings encontrados:**
1. [Warning cr√≠tico] - Severidad: üî¥ - Estado: ‚è≥ Pendiente | ‚úÖ Corregido | üìù Aceptado (raz√≥n: [raz√≥n])
2. [Warning alta] - Severidad: üü° - Estado: ‚è≥ Pendiente | ‚úÖ Corregido | üìù Aceptado (raz√≥n: [raz√≥n])

**Acciones realizadas:**
- [Timestamp] - Ejecutado test del m√≥dulo
- [Timestamp] - [Acci√≥n de correcci√≥n]
- [Timestamp] - Re-ejecutado test despu√©s de correcci√≥n
- [Timestamp] - Investigado test saltado: [nombre]
- [Timestamp] - Clasificado warning: [descripci√≥n]

**Pr√≥ximas acciones:**
- [ ] [Acci√≥n siguiente]
```

---

## üõ†Ô∏è Comandos √ötiles

### Ejecuci√≥n de Tests

```bash
# Ejecutar todos los tests con retroalimentaci√≥n (incluyendo warnings)
cd backend
uv run --extra dev pytest -v --tb=short --durations=10 --timeout=300 -W default::Warning

# Tests de un m√≥dulo espec√≠fico
uv run --extra dev pytest tests/integration/test_[module]_api.py -v

# Tests con cobertura
uv run --extra dev pytest --cov=app --cov-report=html --cov-report=term

# Solo tests fallando (√∫ltima ejecuci√≥n)
uv run --extra dev pytest --lf -v

# Tests marcados (ej: redis)
uv run --extra dev pytest -m "redis" -v

# Tests con timeout individual
uv run --extra dev pytest --timeout=300 -v
```

### An√°lisis de Resultados

```bash
# Contar tests pasando/fallando/saltados y warnings
uv run --extra dev pytest --tb=no -q -W default::Warning | Select-String -Pattern "passed|failed|error|skipped|warning"

# Generar reporte JSON (incluyendo warnings)
uv run --extra dev pytest --json-report --json-report-file=test_report.json -W default::Warning

# Ver tests m√°s lentos
uv run --extra dev pytest --durations=20

# Capturar solo warnings
uv run --extra dev pytest -W default::Warning 2>&1 | Select-String -Pattern "warning"
```

---

## üìö Archivos Clave

- **Helper de tests:** `backend/tests/helpers.py` - `create_user_with_permission()`
- **Event helpers:** `backend/app/core/pubsub/event_helpers.py` - `safe_publish_event()`
- **Configuraci√≥n:** `backend/tests/conftest.py` - Fixtures y setup
- **Reglas:** `rules/tests.md` - Est√°ndares de testing
- **Permisos:** `backend/app/core/auth/permissions.py` - MODULE_ROLES

---

## üéØ Criterios de √âxito Final

- ‚úÖ Todos los tests pasan (0 fallos)
- ‚úÖ Cobertura >90% para m√≥dulos core (auth, permissions, multi-tenancy)
- ‚úÖ Cobertura >80% para m√≥dulos de negocio
- ‚úÖ Todos los endpoints API tienen tests de integraci√≥n
- ‚úÖ Todos los servicios cr√≠ticos tienen tests unitarios
- ‚úÖ Tests validan formato de respuestas seg√∫n API contract
- ‚úÖ Tests incluyen casos edge y validaciones de seguridad
- ‚úÖ **Todos los tests saltados est√°n documentados y justificados**
- ‚úÖ **Tests saltados por problemas han sido corregidos o tienen plan documentado**
- ‚úÖ **Todos los warnings est√°n capturados y clasificados por severidad**
- ‚úÖ **Warnings cr√≠ticas han sido corregidas o tienen plan de correcci√≥n documentado**
- ‚úÖ **Si no se hizo nada con un warning/test saltado, la raz√≥n est√° expl√≠citamente documentada en el informe final**
- ‚úÖ No hay ciclos infinitos de error-cambio-error
- ‚úÖ Documentaci√≥n actualizada
- ‚úÖ Reglas actualizadas si es necesario

---

## üìå Notas Importantes

1. **Actualizar el documento despu√©s de CADA test ejecutado**
2. **Marcar errores como corregidos cuando se solucionen**
3. **Detectar ciclos infinitos y pasar a soluciones de fondo**
4. **Ejecutar suite completa antes de dar por terminado**
5. **Documentar todas las decisiones y cambios realizados**
6. **‚ö†Ô∏è OBLIGATORIO: Investigar y documentar TODOS los tests saltados**
7. **‚ö†Ô∏è OBLIGATORIO: Capturar, clasificar y documentar TODOS los warnings**
8. **‚ö†Ô∏è OBLIGATORIO: Si no se hace nada con un warning/test saltado, explicar expl√≠citamente la raz√≥n en el informe final**

---

## üöÄ Inicio R√°pido

### Comandos para Empezar

```bash
# 1. Crear archivo de seguimiento
cd backend
uv run python tests/scripts/create_test_tracking.py

# 2. Ejecutar suite completa para obtener estado inicial (incluyendo warnings)
uv run --extra dev pytest -v --tb=short --durations=10 --timeout=300 -W default::Warning > initial_test_output.txt 2>&1

# 3. Ver √∫ltimo archivo de seguimiento creado
ls -lt backend/tests/analysis/last_test_*.md | head -1

# 4. Continuar con el primer m√≥dulo del plan
```

### Ejemplo de Flujo por M√≥dulo

```bash
# Ejemplo: M√≥dulo "tags"

# 1. Ejecutar test (incluyendo warnings)
cd backend
uv run --extra dev pytest tests/integration/test_tags_api.py -v --tb=short -W default::Warning > test_tags_output.txt 2>&1

# 2. Ver resultados
cat test_tags_output.txt

# 3. Actualizar documento (manual o con script)
# Opci√≥n A: Manual - Editar last_test_*.md
# Opci√≥n B: Script (si hay errores espec√≠ficos)
uv run python tests/scripts/update_test_tracking.py \
  --module "tags" \
  --test-file "tests/integration/test_tags_api.py" \
  --output "$(cat test_tags_output.txt)"

# 4. Si hay errores, corregirlos y re-ejecutar
# [Aplicar correcciones]
uv run --extra dev pytest tests/integration/test_tags_api.py -v

# 5. Continuar con siguiente m√≥dulo
```

### Comandos de Utilidad

```bash
# Ver progreso actual
cat backend/tests/analysis/last_test_*.md | grep -A 5 "Seguimiento de Progreso"

# Ver errores pendientes
cat backend/tests/analysis/last_test_*.md | grep -A 10 "Errores Pendientes"

# Ver √∫ltimo m√≥dulo procesado
cat backend/tests/analysis/last_test_*.md | grep "### M√≥dulo:" | tail -1

# Contar tests pasando/fallando/saltados y warnings
uv run --extra dev pytest --tb=no -q -W default::Warning | Select-String -Pattern "passed|failed|error|skipped|warning"

---

**√öltima actualizaci√≥n:** [Se actualizar√° autom√°ticamente]
